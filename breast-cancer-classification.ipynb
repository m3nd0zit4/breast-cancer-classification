{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31929715",
   "metadata": {
    "papermill": {
     "duration": 0.003421,
     "end_time": "2025-09-11T22:48:38.646342",
     "exception": false,
     "start_time": "2025-09-11T22:48:38.642921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we explore the **Breast Cancer Wisconsin Diagnostic Dataset**, a well-known dataset commonly used for binary classification tasks.  \n",
    "The dataset contains features computed from digitized images of fine needle aspirates (FNA) of breast masses. Each sample is labeled as either:  \n",
    "- **Benign (B)**: Non-cancerous  \n",
    "- **Malignant (M)**: Cancerous  \n",
    "\n",
    "The goal is to predict whether a tumor is benign or malignant based on these features.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Note on Methodology**  \n",
    "The techniques applied in this notebook go beyond the typical requirements for this dataset.  \n",
    "- We use **advanced preprocessing strategies**,  \n",
    "- **custom loss functions** such as Focal Loss to handle class imbalance,  \n",
    "- and a **deep neural network** with multiple layers, regularization, and optimization tweaks.  \n",
    "\n",
    "While these methods are not strictly necessary for achieving high accuracy on this dataset (since simpler models like Logistic Regression or Random Forests often perform very well), they provide valuable exposure to **state-of-the-art practices in deep learning**.  \n",
    "\n",
    "This makes the notebook a good reference for anyone interested in learning advanced modeling techniques that can be applied to more complex or imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20cf17",
   "metadata": {
    "papermill": {
     "duration": 0.002654,
     "end_time": "2025-09-11T22:48:38.652343",
     "exception": false,
     "start_time": "2025-09-11T22:48:38.649689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the Dataset\n",
    "\n",
    "We begin by importing the **Pandas** library, which is a powerful tool for data manipulation and analysis in Python.  \n",
    "Next, we load the **Breast Cancer dataset** from the given CSV file using `pd.read_csv()`.  \n",
    "The dataset is stored in a DataFrame named `df`.  \n",
    "\n",
    "Finally, by simply typing `df`, we display the contents of the dataset to examine its structure, rows, and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd60838a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-11T22:48:38.659730Z",
     "iopub.status.busy": "2025-09-11T22:48:38.659268Z",
     "iopub.status.idle": "2025-09-11T22:48:40.747011Z",
     "shell.execute_reply": "2025-09-11T22:48:40.746057Z"
    },
    "papermill": {
     "duration": 2.093107,
     "end_time": "2025-09-11T22:48:40.748284",
     "exception": false,
     "start_time": "2025-09-11T22:48:38.655177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"/kaggle/input/breast-cancer-dataset/Breast_cancer_dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c2024",
   "metadata": {
    "papermill": {
     "duration": 0.003177,
     "end_time": "2025-09-11T22:48:40.755310",
     "exception": false,
     "start_time": "2025-09-11T22:48:40.752133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check for Missing Values\n",
    "\n",
    "To ensure data quality, we check for missing values in the dataset.  \n",
    "- `df.isnull().sum()` returns the number of null (missing) entries for each column.  \n",
    "- The result is stored in the variable `valores_nulos`.  \n",
    "- Finally, we print the results to quickly identify if any features contain missing data that may require cleaning or preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350f2b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:48:40.764198Z",
     "iopub.status.busy": "2025-09-11T22:48:40.763911Z",
     "iopub.status.idle": "2025-09-11T22:48:40.772253Z",
     "shell.execute_reply": "2025-09-11T22:48:40.770834Z"
    },
    "papermill": {
     "duration": 0.01446,
     "end_time": "2025-09-11T22:48:40.773790",
     "exception": false,
     "start_time": "2025-09-11T22:48:40.759330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           0\n",
      "diagnosis                    0\n",
      "radius_mean                  0\n",
      "texture_mean                 0\n",
      "perimeter_mean               0\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean          0\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                    0\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                      0\n",
      "smoothness_se                0\n",
      "compactness_se               0\n",
      "concavity_se                 0\n",
      "concave points_se            0\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst                0\n",
      "perimeter_worst              0\n",
      "area_worst                   0\n",
      "smoothness_worst             0\n",
      "compactness_worst            0\n",
      "concavity_worst              0\n",
      "concave points_worst         0\n",
      "symmetry_worst               0\n",
      "fractal_dimension_worst      0\n",
      "Unnamed: 32                569\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "valores_nulos = df.isnull().sum()\n",
    "print(valores_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95db4748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:48:40.781990Z",
     "iopub.status.busy": "2025-09-11T22:48:40.781726Z",
     "iopub.status.idle": "2025-09-11T22:48:54.113701Z",
     "shell.execute_reply": "2025-09-11T22:48:54.112278Z"
    },
    "papermill": {
     "duration": 13.338313,
     "end_time": "2025-09-11T22:48:54.115620",
     "exception": false,
     "start_time": "2025-09-11T22:48:40.777307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.4)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\r\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\r\n",
      "Collecting scikit-learn==1.5.2\r\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Collecting imbalanced-learn==0.12.4\r\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "  Attempting uninstall: imbalanced-learn\r\n",
      "    Found existing installation: imbalanced-learn 0.13.0\r\n",
      "    Uninstalling imbalanced-learn-0.13.0:\r\n",
      "      Successfully uninstalled imbalanced-learn-0.13.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed imbalanced-learn-0.12.4 scikit-learn-1.5.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy pandas\n",
    "!pip install scikit-learn==1.5.2 imbalanced-learn==0.12.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0d3b5",
   "metadata": {
    "papermill": {
     "duration": 0.004428,
     "end_time": "2025-09-11T22:48:54.125343",
     "exception": false,
     "start_time": "2025-09-11T22:48:54.120915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install and Import Required Libraries\n",
    "\n",
    "Before building and training our deep learning model, we need to install and import the necessary libraries.\n",
    "\n",
    "- `tensorflow`: Provides tools for building and training neural networks.  \n",
    "- `numpy`: Used for numerical computations and handling arrays.  \n",
    "- `pandas`: Essential for data manipulation and analysis.  \n",
    "- `scikit-learn`: A powerful machine learning library for preprocessing, model selection, and evaluation.  \n",
    "- `imbalanced-learn`: Provides resampling techniques such as **SMOTE** to handle class imbalance.  \n",
    "\n",
    "We also import specific modules from these libraries, including:  \n",
    "- **Data preprocessing tools** (`StandardScaler`, `LabelEncoder`, `OneHotEncoder`, `ColumnTransformer`)  \n",
    "- **Class balancing utility** (`compute_class_weight`)  \n",
    "- **Oversampling technique** (`SMOTE`)  \n",
    "- **Keras API for building models** (`Sequential`, `Dense`, `Dropout`, etc.)  \n",
    "- **Optimizers** (`Adam`, `Nadam`)  \n",
    "- **Callbacks** for training control (`EarlyStopping`, `ReduceLROnPlateau`, `ModelCheckpoint`)  \n",
    "- **Regularizers** (`l1_l2`)  \n",
    "- **Evaluation metrics** (`AUC`, `Precision`, `Recall`, etc.)  \n",
    "- **Initializers** (`HeNormal`)  \n",
    "\n",
    "These tools will allow us to construct, train, and evaluate a robust deep learning model for the breast cancer dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97aa5eba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:48:54.136026Z",
     "iopub.status.busy": "2025-09-11T22:48:54.135733Z",
     "iopub.status.idle": "2025-09-11T22:49:12.742242Z",
     "shell.execute_reply": "2025-09-11T22:49:12.741424Z"
    },
    "papermill": {
     "duration": 18.613851,
     "end_time": "2025-09-11T22:49:12.743818",
     "exception": false,
     "start_time": "2025-09-11T22:48:54.129967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 22:48:56.122518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757630936.379555      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757630936.446028      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, TruePositives, TrueNegatives\n",
    "from tensorflow.keras.initializers import HeNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498a094",
   "metadata": {
    "papermill": {
     "duration": 0.004415,
     "end_time": "2025-09-11T22:49:12.753086",
     "exception": false,
     "start_time": "2025-09-11T22:49:12.748671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "1. **Load and clean the dataset**  \n",
    "   - Read the CSV file into a Pandas DataFrame.  \n",
    "   - Drop unnecessary columns: `Unnamed: 32` (all null values) and `id` (identifier not useful for modeling).  \n",
    "\n",
    "2. **Check class distribution**  \n",
    "   - Count the number of samples in each class (`diagnosis`) to verify if the dataset is balanced.  \n",
    "\n",
    "3. **Define features (`X`) and target (`y`)**  \n",
    "   - `X`: All columns except `diagnosis`.  \n",
    "   - `y`: The target column `diagnosis`, which indicates tumor type (*Benign* or *Malignant*).  \n",
    "\n",
    "4. **Encode the target variable**  \n",
    "   - Use `LabelEncoder` to convert categorical labels into numeric values:  \n",
    "     - `B` → 0  \n",
    "     - `M` → 1  \n",
    "\n",
    "5. **Split the dataset**  \n",
    "   - Divide the data into training (80%) and testing (20%) sets.  \n",
    "   - Use `stratify=y` to preserve the class proportion in both sets.  \n",
    "\n",
    "6. **Define columns for preprocessing**  \n",
    "   - Numerical features: All measurement-related columns.  \n",
    "   - Categorical features: None in this dataset.  \n",
    "\n",
    "7. **Preprocess the features**  \n",
    "   - Apply **StandardScaler** to normalize numerical features.  \n",
    "   - Use `ColumnTransformer` to ensure transformations are applied correctly.  \n",
    "\n",
    "8. **Fit and transform the data**  \n",
    "   - Fit the preprocessor on the training set.  \n",
    "   - Transform both training and testing sets to obtain scaled versions.  \n",
    "\n",
    "9. **Verify preprocessing results**  \n",
    "   - Print the shapes of the original and processed datasets.  \n",
    "   - Confirm the output data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa75e3ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:49:12.764455Z",
     "iopub.status.busy": "2025-09-11T22:49:12.763233Z",
     "iopub.status.idle": "2025-09-11T22:49:12.845295Z",
     "shell.execute_reply": "2025-09-11T22:49:12.844237Z"
    },
    "papermill": {
     "duration": 0.089321,
     "end_time": "2025-09-11T22:49:12.846932",
     "exception": false,
     "start_time": "2025-09-11T22:49:12.757611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "diagnosis\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n",
      "{'B': 0, 'M': 1}\n",
      "\n",
      "Verificación final:\n",
      "X_train original: (455, 30)\n",
      "X_train procesado: (455, 30)\n",
      "X_test original: (114, 30)\n",
      "X_test procesado: (114, 30)\n",
      "Tipos de datos: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/kaggle/input/breast-cancer-dataset/Breast_cancer_dataset.csv\")\n",
    "\n",
    "#Borrar los datos nulos\n",
    "df = df.drop(columns=[\"Unnamed: 32\", \"id\"])\n",
    "\n",
    "#Verificar balance de clases\n",
    "class_counts = df['diagnosis'].value_counts()\n",
    "print(f\"Distribución de clases:\\n{class_counts}\")\n",
    "df\n",
    "\n",
    "#Definir X e y\n",
    "X = df.drop(columns=[\"diagnosis\"])\n",
    "y = df[\"diagnosis\"]\n",
    "\n",
    "# Convertir target a 0/1\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # 'B' → 0, 'M' → 1 (o viceversa)\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "#Dividir datos PRIMERO (antes de cualquier procesamiento)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Mantener proporción de clases\n",
    ")\n",
    "\n",
    "#Definir columnas para transformación\n",
    "numerical_cols = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean',             \n",
    "'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se',                 \n",
    "'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se',         \n",
    "'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst',              \n",
    "'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
    "\n",
    "categorical_cols = []\n",
    "\n",
    "#Crear preprocesador\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Transformador numérico: 3 elementos\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ],\n",
    "    sparse_threshold=0  # Forzar salida densa\n",
    ")\n",
    "\n",
    "#Aplicar transformaciones DESPUÉS de dividir\n",
    "X_train_processed = preprocessor.fit_transform(X_train)  # Solo fit en train\n",
    "X_test_processed = preprocessor.transform(X_test)        # Solo transform en test\n",
    "\n",
    "#Verificar resultados\n",
    "print(\"\\nVerificación final:\")\n",
    "print(f\"X_train original: {X_train.shape}\")\n",
    "print(f\"X_train procesado: {X_train_processed.shape}\")\n",
    "print(f\"X_test original: {X_test.shape}\")\n",
    "print(f\"X_test procesado: {X_test_processed.shape}\")\n",
    "print(f\"Tipos de datos: {type(X_train_processed)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d708d31",
   "metadata": {
    "papermill": {
     "duration": 0.005887,
     "end_time": "2025-09-11T22:49:12.858110",
     "exception": false,
     "start_time": "2025-09-11T22:49:12.852223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Definition and Training\n",
    "\n",
    "### 1. Training Base Data\n",
    "We define the processed training data (`X_train_base`) and its corresponding labels (`y_train_base`) as the inputs for model training.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Custom Focal Loss Function\n",
    "To address the class imbalance problem, we implement an **improved Focal Loss** function:  \n",
    "- **Focal Loss** dynamically scales the cross-entropy loss, giving more weight to hard-to-classify examples.  \n",
    "- `gamma`: Controls how strongly difficult samples are emphasized.  \n",
    "- `alpha`: Balances the contribution of positive vs. negative classes.  \n",
    "- The function is designed with numerical stability improvements by clipping predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Model Architecture\n",
    "We build a deep neural network using **Keras Sequential API** with the following structure:  \n",
    "\n",
    "- **Input Layer**  \n",
    "  - `Dense(256)` with HeNormal initialization and L1/L2 regularization.  \n",
    "  - `BatchNormalization` for faster convergence.  \n",
    "  - `Activation('swish')`, which often performs better than ReLU.  \n",
    "  - `Dropout(0.6)` for strong regularization.  \n",
    "\n",
    "- **Hidden Layers**  \n",
    "  - `Dense(128)` + BatchNorm + Swish + Dropout(0.5).  \n",
    "  - `Dense(64)` + BatchNorm + Swish + Dropout(0.4).  \n",
    "  - `Dense(32)` + BatchNorm + Swish + Dropout(0.3).  \n",
    "\n",
    "- **Output Layer**  \n",
    "  - `Dense(1, activation='sigmoid')` for binary classification (benign vs. malignant).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Optimizer and Compilation\n",
    "- Optimizer: **Nadam** (a variant of Adam with Nesterov momentum), configured with:  \n",
    "  - Learning rate = `0.0001`  \n",
    "  - Gradient clipping (`clipnorm=1.0`) to avoid exploding gradients.  \n",
    "\n",
    "- Loss: Custom **Focal Loss** (`gamma=2.5`, `alpha=0.65`).  \n",
    "- Metrics:  \n",
    "  - Accuracy  \n",
    "  - Precision  \n",
    "  - Recall  \n",
    "  - AUC (Area Under the ROC Curve)  \n",
    "  - True Positives (`tp`)  \n",
    "  - True Negatives (`tn`)  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Model Training\n",
    "We train the model with the following setup:  \n",
    "- Training data: `X_train_base`, `y_train_base`.  \n",
    "- Validation split: 15% of the training set is used for validation.  \n",
    "- Epochs: `150` for extended training.  \n",
    "- Shuffling enabled at each epoch to improve generalization.  \n",
    "\n",
    "The training history (`history`) stores performance metrics for both training and validation sets across epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54abc6a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:49:12.870332Z",
     "iopub.status.busy": "2025-09-11T22:49:12.869631Z",
     "iopub.status.idle": "2025-09-11T22:49:52.435272Z",
     "shell.execute_reply": "2025-09-11T22:49:52.434210Z"
    },
    "papermill": {
     "duration": 39.573274,
     "end_time": "2025-09-11T22:49:52.436834",
     "exception": false,
     "start_time": "2025-09-11T22:49:12.863560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-09-11 22:49:12.884266: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 125ms/step - accuracy: 0.4877 - auc: 0.5068 - loss: 0.2209 - precision: 0.3944 - recall: 0.4389 - tn: 71.9286 - tp: 42.5000 - val_accuracy: 0.4203 - val_auc: 0.4232 - val_loss: 0.1497 - val_precision: 0.2000 - val_recall: 0.3684 - val_tn: 22.0000 - val_tp: 7.0000\n",
      "Epoch 2/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5203 - auc: 0.5531 - loss: 0.2090 - precision: 0.4113 - recall: 0.5202 - tn: 70.2143 - tp: 46.3571 - val_accuracy: 0.4493 - val_auc: 0.5021 - val_loss: 0.1365 - val_precision: 0.2286 - val_recall: 0.4211 - val_tn: 23.0000 - val_tp: 8.0000\n",
      "Epoch 3/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5825 - auc: 0.5512 - loss: 0.2131 - precision: 0.4894 - recall: 0.6375 - tn: 73.5714 - tp: 57.4286 - val_accuracy: 0.5797 - val_auc: 0.6542 - val_loss: 0.1271 - val_precision: 0.3684 - val_recall: 0.7368 - val_tn: 26.0000 - val_tp: 14.0000\n",
      "Epoch 4/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4961 - auc: 0.5227 - loss: 0.1964 - precision: 0.3946 - recall: 0.4676 - tn: 73.0000 - tp: 45.1429 - val_accuracy: 0.6232 - val_auc: 0.7726 - val_loss: 0.1208 - val_precision: 0.4054 - val_recall: 0.7895 - val_tn: 28.0000 - val_tp: 15.0000\n",
      "Epoch 5/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4993 - auc: 0.4812 - loss: 0.2282 - precision: 0.3690 - recall: 0.5428 - tn: 69.2857 - tp: 49.8571 - val_accuracy: 0.6812 - val_auc: 0.8389 - val_loss: 0.1171 - val_precision: 0.4595 - val_recall: 0.8947 - val_tn: 30.0000 - val_tp: 17.0000\n",
      "Epoch 6/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5737 - auc: 0.5761 - loss: 0.1845 - precision: 0.4419 - recall: 0.5521 - tn: 81.9286 - tp: 50.4286 - val_accuracy: 0.6957 - val_auc: 0.8916 - val_loss: 0.1136 - val_precision: 0.4722 - val_recall: 0.8947 - val_tn: 31.0000 - val_tp: 17.0000\n",
      "Epoch 7/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5286 - auc: 0.5805 - loss: 0.1795 - precision: 0.4210 - recall: 0.5098 - tn: 75.1429 - tp: 47.5714 - val_accuracy: 0.7101 - val_auc: 0.9158 - val_loss: 0.1116 - val_precision: 0.4857 - val_recall: 0.8947 - val_tn: 32.0000 - val_tp: 17.0000\n",
      "Epoch 8/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5866 - auc: 0.6453 - loss: 0.1704 - precision: 0.4766 - recall: 0.6482 - tn: 81.2143 - tp: 57.7857 - val_accuracy: 0.7246 - val_auc: 0.9316 - val_loss: 0.1094 - val_precision: 0.5000 - val_recall: 0.8947 - val_tn: 33.0000 - val_tp: 17.0000\n",
      "Epoch 9/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5847 - auc: 0.6330 - loss: 0.1900 - precision: 0.4867 - recall: 0.5923 - tn: 83.0000 - tp: 55.7857 - val_accuracy: 0.7681 - val_auc: 0.9463 - val_loss: 0.1074 - val_precision: 0.5484 - val_recall: 0.8947 - val_tn: 36.0000 - val_tp: 17.0000\n",
      "Epoch 10/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6380 - auc: 0.6850 - loss: 0.1823 - precision: 0.5943 - recall: 0.6335 - tn: 84.0000 - tp: 62.1429 - val_accuracy: 0.7971 - val_auc: 0.9537 - val_loss: 0.1060 - val_precision: 0.5806 - val_recall: 0.9474 - val_tn: 37.0000 - val_tp: 18.0000\n",
      "Epoch 11/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6290 - auc: 0.6952 - loss: 0.1763 - precision: 0.5357 - recall: 0.6683 - tn: 85.6429 - tp: 61.1429 - val_accuracy: 0.8551 - val_auc: 0.9621 - val_loss: 0.1045 - val_precision: 0.6667 - val_recall: 0.9474 - val_tn: 41.0000 - val_tp: 18.0000\n",
      "Epoch 12/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6204 - auc: 0.6397 - loss: 0.1747 - precision: 0.5063 - recall: 0.6521 - tn: 87.3571 - tp: 58.9286 - val_accuracy: 0.8551 - val_auc: 0.9658 - val_loss: 0.1035 - val_precision: 0.6667 - val_recall: 0.9474 - val_tn: 41.0000 - val_tp: 18.0000\n",
      "Epoch 13/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6770 - auc: 0.7154 - loss: 0.1631 - precision: 0.5899 - recall: 0.6844 - tn: 93.2143 - tp: 63.7857 - val_accuracy: 0.8551 - val_auc: 0.9726 - val_loss: 0.1027 - val_precision: 0.6667 - val_recall: 0.9474 - val_tn: 41.0000 - val_tp: 18.0000\n",
      "Epoch 14/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6652 - auc: 0.7257 - loss: 0.1496 - precision: 0.5483 - recall: 0.6771 - tn: 97.9286 - tp: 60.9286 - val_accuracy: 0.8551 - val_auc: 0.9784 - val_loss: 0.1017 - val_precision: 0.6667 - val_recall: 0.9474 - val_tn: 41.0000 - val_tp: 18.0000\n",
      "Epoch 15/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7081 - auc: 0.7661 - loss: 0.1409 - precision: 0.5955 - recall: 0.7241 - tn: 100.0714 - tp: 67.6429 - val_accuracy: 0.8696 - val_auc: 0.9789 - val_loss: 0.1010 - val_precision: 0.6923 - val_recall: 0.9474 - val_tn: 42.0000 - val_tp: 18.0000\n",
      "Epoch 16/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7267 - auc: 0.7949 - loss: 0.1478 - precision: 0.6073 - recall: 0.7339 - tn: 103.9286 - tp: 62.8571 - val_accuracy: 0.8696 - val_auc: 0.9789 - val_loss: 0.0997 - val_precision: 0.6923 - val_recall: 0.9474 - val_tn: 42.0000 - val_tp: 18.0000\n",
      "Epoch 17/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6857 - auc: 0.7369 - loss: 0.1495 - precision: 0.5600 - recall: 0.6646 - tn: 99.0714 - tp: 59.7857 - val_accuracy: 0.8696 - val_auc: 0.9805 - val_loss: 0.0988 - val_precision: 0.6923 - val_recall: 0.9474 - val_tn: 42.0000 - val_tp: 18.0000\n",
      "Epoch 18/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6745 - auc: 0.7168 - loss: 0.1521 - precision: 0.5301 - recall: 0.6751 - tn: 98.1429 - tp: 59.2143 - val_accuracy: 0.8696 - val_auc: 0.9832 - val_loss: 0.0984 - val_precision: 0.6923 - val_recall: 0.9474 - val_tn: 42.0000 - val_tp: 18.0000\n",
      "Epoch 19/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7327 - auc: 0.7987 - loss: 0.1312 - precision: 0.6356 - recall: 0.7405 - tn: 103.2143 - tp: 65.0714 - val_accuracy: 0.8696 - val_auc: 0.9842 - val_loss: 0.0976 - val_precision: 0.6923 - val_recall: 0.9474 - val_tn: 42.0000 - val_tp: 18.0000\n",
      "Epoch 20/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7618 - auc: 0.8148 - loss: 0.1363 - precision: 0.6514 - recall: 0.7581 - tn: 108.2143 - tp: 67.7857 - val_accuracy: 0.8696 - val_auc: 0.9853 - val_loss: 0.0971 - val_precision: 0.6923 - val_recall: 0.9474 - val_tn: 42.0000 - val_tp: 18.0000\n",
      "Epoch 21/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7620 - auc: 0.8194 - loss: 0.1388 - precision: 0.6925 - recall: 0.7256 - tn: 109.9286 - tp: 67.2143 - val_accuracy: 0.8841 - val_auc: 0.9853 - val_loss: 0.0967 - val_precision: 0.7200 - val_recall: 0.9474 - val_tn: 43.0000 - val_tp: 18.0000\n",
      "Epoch 22/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7208 - auc: 0.7954 - loss: 0.1461 - precision: 0.6657 - recall: 0.7009 - tn: 101.9286 - tp: 68.7857 - val_accuracy: 0.8986 - val_auc: 0.9874 - val_loss: 0.0960 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 23/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7105 - auc: 0.7989 - loss: 0.1353 - precision: 0.5827 - recall: 0.7475 - tn: 103.8571 - tp: 65.2857 - val_accuracy: 0.8986 - val_auc: 0.9874 - val_loss: 0.0956 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 24/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7500 - auc: 0.8460 - loss: 0.1261 - precision: 0.6648 - recall: 0.7698 - tn: 102.5000 - tp: 71.6429 - val_accuracy: 0.8986 - val_auc: 0.9884 - val_loss: 0.0949 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 25/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7465 - auc: 0.8441 - loss: 0.1300 - precision: 0.6644 - recall: 0.7611 - tn: 104.3571 - tp: 70.0000 - val_accuracy: 0.8986 - val_auc: 0.9884 - val_loss: 0.0946 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 26/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6995 - auc: 0.7945 - loss: 0.1370 - precision: 0.5784 - recall: 0.7329 - tn: 101.5000 - tp: 64.2143 - val_accuracy: 0.8986 - val_auc: 0.9889 - val_loss: 0.0942 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 27/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7521 - auc: 0.8357 - loss: 0.1246 - precision: 0.6637 - recall: 0.7508 - tn: 107.3571 - tp: 71.9286 - val_accuracy: 0.8986 - val_auc: 0.9889 - val_loss: 0.0937 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 28/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8280 - auc: 0.8772 - loss: 0.1165 - precision: 0.7698 - recall: 0.8177 - tn: 117.0714 - tp: 76.2857 - val_accuracy: 0.8986 - val_auc: 0.9884 - val_loss: 0.0934 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 29/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7293 - auc: 0.8177 - loss: 0.1264 - precision: 0.6407 - recall: 0.7039 - tn: 104.7143 - tp: 68.2857 - val_accuracy: 0.8986 - val_auc: 0.9895 - val_loss: 0.0933 - val_precision: 0.7500 - val_recall: 0.9474 - val_tn: 44.0000 - val_tp: 18.0000\n",
      "Epoch 30/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7623 - auc: 0.8434 - loss: 0.1312 - precision: 0.6736 - recall: 0.7756 - tn: 104.5000 - tp: 71.2143 - val_accuracy: 0.9130 - val_auc: 0.9895 - val_loss: 0.0927 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 31/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7929 - auc: 0.8626 - loss: 0.1183 - precision: 0.7010 - recall: 0.8133 - tn: 112.1429 - tp: 72.5714 - val_accuracy: 0.9130 - val_auc: 0.9900 - val_loss: 0.0925 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 32/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7758 - auc: 0.8597 - loss: 0.1234 - precision: 0.6610 - recall: 0.8393 - tn: 106.9286 - tp: 73.7857 - val_accuracy: 0.9130 - val_auc: 0.9905 - val_loss: 0.0925 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 33/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7693 - auc: 0.8714 - loss: 0.1225 - precision: 0.6455 - recall: 0.8494 - tn: 105.7143 - tp: 76.0714 - val_accuracy: 0.9130 - val_auc: 0.9905 - val_loss: 0.0921 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 34/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7666 - auc: 0.8388 - loss: 0.1313 - precision: 0.6797 - recall: 0.7568 - tn: 109.2857 - tp: 69.5000 - val_accuracy: 0.9130 - val_auc: 0.9905 - val_loss: 0.0918 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 35/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8033 - auc: 0.8504 - loss: 0.1252 - precision: 0.7244 - recall: 0.7745 - tn: 115.3571 - tp: 71.6429 - val_accuracy: 0.9130 - val_auc: 0.9905 - val_loss: 0.0916 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 36/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7916 - auc: 0.8754 - loss: 0.1150 - precision: 0.6841 - recall: 0.8348 - tn: 112.6429 - tp: 73.4286 - val_accuracy: 0.9130 - val_auc: 0.9911 - val_loss: 0.0911 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 37/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8148 - auc: 0.8881 - loss: 0.1219 - precision: 0.7500 - recall: 0.8292 - tn: 112.3571 - tp: 77.8571 - val_accuracy: 0.9130 - val_auc: 0.9911 - val_loss: 0.0909 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 38/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8362 - auc: 0.9017 - loss: 0.1120 - precision: 0.7579 - recall: 0.8647 - tn: 113.2857 - tp: 78.3571 - val_accuracy: 0.9130 - val_auc: 0.9911 - val_loss: 0.0908 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 39/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8202 - auc: 0.8880 - loss: 0.1162 - precision: 0.7606 - recall: 0.8238 - tn: 115.9286 - tp: 75.4286 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0903 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 40/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8007 - auc: 0.8662 - loss: 0.1178 - precision: 0.7221 - recall: 0.7805 - tn: 115.9286 - tp: 68.2143 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0902 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 41/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8400 - auc: 0.9310 - loss: 0.1047 - precision: 0.7642 - recall: 0.8669 - tn: 114.7143 - tp: 79.3571 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0901 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 42/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7974 - auc: 0.8624 - loss: 0.1198 - precision: 0.6934 - recall: 0.7975 - tn: 114.7857 - tp: 72.2857 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0898 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 43/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7895 - auc: 0.9039 - loss: 0.1087 - precision: 0.7075 - recall: 0.7888 - tn: 110.6429 - tp: 71.6429 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0896 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 44/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8089 - auc: 0.8966 - loss: 0.1118 - precision: 0.7206 - recall: 0.8207 - tn: 115.0714 - tp: 73.9286 - val_accuracy: 0.9275 - val_auc: 0.9916 - val_loss: 0.0891 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 45/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8154 - auc: 0.8922 - loss: 0.1130 - precision: 0.7288 - recall: 0.8488 - tn: 112.3571 - tp: 79.2143 - val_accuracy: 0.9275 - val_auc: 0.9916 - val_loss: 0.0890 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 46/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8142 - auc: 0.8873 - loss: 0.1145 - precision: 0.7016 - recall: 0.8614 - tn: 114.7857 - tp: 74.7143 - val_accuracy: 0.9275 - val_auc: 0.9911 - val_loss: 0.0891 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 47/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7947 - auc: 0.9223 - loss: 0.1123 - precision: 0.6488 - recall: 0.8931 - tn: 116.1429 - tp: 75.7143 - val_accuracy: 0.9275 - val_auc: 0.9916 - val_loss: 0.0890 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 48/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7888 - auc: 0.8791 - loss: 0.1163 - precision: 0.7337 - recall: 0.7739 - tn: 111.0000 - tp: 75.7857 - val_accuracy: 0.9275 - val_auc: 0.9916 - val_loss: 0.0888 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 49/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8582 - auc: 0.9266 - loss: 0.1054 - precision: 0.7748 - recall: 0.8997 - tn: 118.9286 - tp: 82.2143 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0888 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 50/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8356 - auc: 0.9039 - loss: 0.1098 - precision: 0.7558 - recall: 0.8553 - tn: 117.7143 - tp: 78.3571 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0887 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 51/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7934 - auc: 0.8899 - loss: 0.1125 - precision: 0.7157 - recall: 0.8153 - tn: 108.1429 - tp: 77.3571 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0887 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 52/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8282 - auc: 0.9092 - loss: 0.1071 - precision: 0.7263 - recall: 0.8488 - tn: 117.5714 - tp: 76.7857 - val_accuracy: 0.9130 - val_auc: 0.9916 - val_loss: 0.0884 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 53/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8494 - auc: 0.9268 - loss: 0.1060 - precision: 0.7834 - recall: 0.8512 - tn: 120.7857 - tp: 77.2857 - val_accuracy: 0.9130 - val_auc: 0.9921 - val_loss: 0.0883 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 54/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8592 - auc: 0.9201 - loss: 0.1107 - precision: 0.8126 - recall: 0.8651 - tn: 116.5714 - tp: 81.3571 - val_accuracy: 0.9130 - val_auc: 0.9926 - val_loss: 0.0881 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 55/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8542 - auc: 0.9313 - loss: 0.1105 - precision: 0.7580 - recall: 0.9013 - tn: 119.3571 - tp: 78.6429 - val_accuracy: 0.9130 - val_auc: 0.9921 - val_loss: 0.0879 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 56/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8237 - auc: 0.9063 - loss: 0.1116 - precision: 0.7217 - recall: 0.8309 - tn: 122.7857 - tp: 70.9286 - val_accuracy: 0.9275 - val_auc: 0.9926 - val_loss: 0.0876 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 57/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8425 - auc: 0.9308 - loss: 0.1060 - precision: 0.7482 - recall: 0.8871 - tn: 116.2857 - tp: 78.8571 - val_accuracy: 0.9275 - val_auc: 0.9932 - val_loss: 0.0874 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 58/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8686 - auc: 0.9371 - loss: 0.1056 - precision: 0.7649 - recall: 0.9328 - tn: 120.2143 - tp: 82.2143 - val_accuracy: 0.9275 - val_auc: 0.9932 - val_loss: 0.0872 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 59/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8969 - auc: 0.9517 - loss: 0.0989 - precision: 0.8243 - recall: 0.9196 - tn: 126.9286 - tp: 80.8571 - val_accuracy: 0.9275 - val_auc: 0.9926 - val_loss: 0.0871 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 60/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8535 - auc: 0.9459 - loss: 0.1019 - precision: 0.7613 - recall: 0.9121 - tn: 116.4286 - tp: 82.5000 - val_accuracy: 0.9275 - val_auc: 0.9926 - val_loss: 0.0871 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 61/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8229 - auc: 0.9146 - loss: 0.1196 - precision: 0.6960 - recall: 0.8561 - tn: 123.2143 - tp: 74.0714 - val_accuracy: 0.9275 - val_auc: 0.9932 - val_loss: 0.0870 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 62/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8687 - auc: 0.9301 - loss: 0.1059 - precision: 0.7952 - recall: 0.8848 - tn: 122.3571 - tp: 79.7143 - val_accuracy: 0.9275 - val_auc: 0.9932 - val_loss: 0.0869 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 63/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8238 - auc: 0.9097 - loss: 0.1103 - precision: 0.7456 - recall: 0.8378 - tn: 118.5000 - tp: 75.6429 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0868 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 64/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8095 - auc: 0.9050 - loss: 0.1120 - precision: 0.6818 - recall: 0.8610 - tn: 119.3571 - tp: 74.2857 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0866 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 65/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8766 - auc: 0.9568 - loss: 0.0978 - precision: 0.7938 - recall: 0.9218 - tn: 121.0714 - tp: 80.9286 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0866 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 66/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8510 - auc: 0.9275 - loss: 0.1057 - precision: 0.7676 - recall: 0.8671 - tn: 121.4286 - tp: 76.3571 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0865 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 67/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8452 - auc: 0.9324 - loss: 0.1082 - precision: 0.7838 - recall: 0.8662 - tn: 117.7143 - tp: 82.0714 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0865 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 68/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8483 - auc: 0.9177 - loss: 0.1093 - precision: 0.7622 - recall: 0.8859 - tn: 119.5000 - tp: 80.7143 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0865 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 69/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8564 - auc: 0.9326 - loss: 0.1033 - precision: 0.8107 - recall: 0.8406 - tn: 124.7143 - tp: 79.1429 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0863 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 70/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8865 - auc: 0.9422 - loss: 0.1035 - precision: 0.8129 - recall: 0.9158 - tn: 125.8571 - tp: 82.1429 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0862 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 71/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8871 - auc: 0.9463 - loss: 0.1050 - precision: 0.8378 - recall: 0.9015 - tn: 120.7143 - tp: 84.2143 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0861 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 72/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8503 - auc: 0.9136 - loss: 0.1112 - precision: 0.7856 - recall: 0.8379 - tn: 120.8571 - tp: 76.7857 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0860 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 73/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8521 - auc: 0.9433 - loss: 0.1004 - precision: 0.7754 - recall: 0.8965 - tn: 115.4286 - tp: 84.1429 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0860 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 74/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8539 - auc: 0.9332 - loss: 0.1038 - precision: 0.7938 - recall: 0.8746 - tn: 116.2857 - tp: 82.5000 - val_accuracy: 0.9130 - val_auc: 0.9937 - val_loss: 0.0859 - val_precision: 0.7826 - val_recall: 0.9474 - val_tn: 45.0000 - val_tp: 18.0000\n",
      "Epoch 75/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8586 - auc: 0.9476 - loss: 0.0993 - precision: 0.7520 - recall: 0.9073 - tn: 123.6429 - tp: 78.0714 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0857 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 76/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8616 - auc: 0.9392 - loss: 0.1112 - precision: 0.7828 - recall: 0.8883 - tn: 119.4286 - tp: 79.9286 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0858 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 77/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8660 - auc: 0.9127 - loss: 0.1085 - precision: 0.8044 - recall: 0.8619 - tn: 126.5000 - tp: 79.0000 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0855 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 78/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8778 - auc: 0.9567 - loss: 0.0972 - precision: 0.7943 - recall: 0.9071 - tn: 124.7857 - tp: 78.2857 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0854 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 79/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8879 - auc: 0.9599 - loss: 0.0969 - precision: 0.8345 - recall: 0.9078 - tn: 119.5000 - tp: 85.2143 - val_accuracy: 0.9275 - val_auc: 0.9937 - val_loss: 0.0853 - val_precision: 0.8182 - val_recall: 0.9474 - val_tn: 46.0000 - val_tp: 18.0000\n",
      "Epoch 80/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8905 - auc: 0.9471 - loss: 0.0997 - precision: 0.8297 - recall: 0.9084 - tn: 125.3571 - tp: 81.7143 - val_accuracy: 0.9565 - val_auc: 0.9937 - val_loss: 0.0850 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 81/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8845 - auc: 0.9514 - loss: 0.0989 - precision: 0.8318 - recall: 0.8892 - tn: 123.3571 - tp: 79.7143 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0850 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 82/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8302 - auc: 0.9346 - loss: 0.1023 - precision: 0.7217 - recall: 0.8908 - tn: 115.7857 - tp: 79.7857 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0850 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 83/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8478 - auc: 0.9340 - loss: 0.1083 - precision: 0.7322 - recall: 0.9049 - tn: 120.6429 - tp: 77.6429 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0850 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 84/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8841 - auc: 0.9468 - loss: 0.1022 - precision: 0.8171 - recall: 0.9199 - tn: 121.5714 - tp: 85.1429 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0849 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 85/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9053 - auc: 0.9634 - loss: 0.0950 - precision: 0.8274 - recall: 0.9409 - tn: 128.5000 - tp: 81.7857 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0850 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 86/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8815 - auc: 0.9521 - loss: 0.1004 - precision: 0.8347 - recall: 0.8572 - tn: 128.4286 - tp: 78.6429 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0848 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 87/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8840 - auc: 0.9351 - loss: 0.1021 - precision: 0.8261 - recall: 0.8744 - tn: 126.7143 - tp: 78.7143 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0847 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 88/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8423 - auc: 0.9344 - loss: 0.1050 - precision: 0.7917 - recall: 0.8020 - tn: 123.2857 - tp: 73.5714 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0846 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 89/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8702 - auc: 0.9389 - loss: 0.1037 - precision: 0.8161 - recall: 0.8699 - tn: 123.8571 - tp: 79.9286 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0847 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 90/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8507 - auc: 0.9472 - loss: 0.1005 - precision: 0.7315 - recall: 0.9084 - tn: 121.8571 - tp: 77.4286 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0848 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 91/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8991 - auc: 0.9575 - loss: 0.0982 - precision: 0.8278 - recall: 0.9330 - tn: 126.6429 - tp: 83.7143 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0847 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 92/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8866 - auc: 0.9627 - loss: 0.0950 - precision: 0.8398 - recall: 0.8878 - tn: 123.2857 - tp: 82.4286 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0848 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 93/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8831 - auc: 0.9602 - loss: 0.0992 - precision: 0.7891 - recall: 0.9211 - tn: 127.0714 - tp: 79.2857 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0847 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 94/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8741 - auc: 0.9444 - loss: 0.1001 - precision: 0.8066 - recall: 0.8848 - tn: 126.0714 - tp: 79.3571 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0843 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 95/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8637 - auc: 0.9438 - loss: 0.1024 - precision: 0.7798 - recall: 0.8823 - tn: 124.0714 - tp: 78.0714 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0840 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 96/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8570 - auc: 0.9443 - loss: 0.1001 - precision: 0.7992 - recall: 0.8438 - tn: 123.3571 - tp: 78.0000 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0840 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 97/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9188 - auc: 0.9641 - loss: 0.0962 - precision: 0.8643 - recall: 0.9452 - tn: 125.5000 - tp: 87.3571 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0840 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 98/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8929 - auc: 0.9567 - loss: 0.0971 - precision: 0.8248 - recall: 0.9024 - tn: 129.7143 - tp: 79.0000 - val_accuracy: 0.9420 - val_auc: 0.9937 - val_loss: 0.0841 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 99/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8890 - auc: 0.9535 - loss: 0.0965 - precision: 0.8316 - recall: 0.9071 - tn: 124.2143 - tp: 85.2143 - val_accuracy: 0.9420 - val_auc: 0.9942 - val_loss: 0.0842 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 100/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9228 - auc: 0.9741 - loss: 0.0914 - precision: 0.8826 - recall: 0.9243 - tn: 132.2857 - tp: 82.8571 - val_accuracy: 0.9420 - val_auc: 0.9942 - val_loss: 0.0842 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 101/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8957 - auc: 0.9619 - loss: 0.0973 - precision: 0.8296 - recall: 0.9329 - tn: 123.2857 - tp: 86.5714 - val_accuracy: 0.9420 - val_auc: 0.9942 - val_loss: 0.0841 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 102/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8677 - auc: 0.9513 - loss: 0.0984 - precision: 0.7637 - recall: 0.8873 - tn: 125.9286 - tp: 78.9286 - val_accuracy: 0.9420 - val_auc: 0.9942 - val_loss: 0.0841 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 103/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8739 - auc: 0.9411 - loss: 0.1048 - precision: 0.8692 - recall: 0.8423 - tn: 122.2143 - tp: 82.6429 - val_accuracy: 0.9420 - val_auc: 0.9942 - val_loss: 0.0837 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 104/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8938 - auc: 0.9446 - loss: 0.1007 - precision: 0.8436 - recall: 0.8748 - tn: 128.9286 - tp: 78.4286 - val_accuracy: 0.9420 - val_auc: 0.9942 - val_loss: 0.0838 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 105/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9058 - auc: 0.9669 - loss: 0.0929 - precision: 0.8705 - recall: 0.9028 - tn: 125.9286 - tp: 85.8571 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0837 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 106/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8977 - auc: 0.9655 - loss: 0.0972 - precision: 0.8307 - recall: 0.9239 - tn: 126.2143 - tp: 84.6429 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0838 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 107/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9166 - auc: 0.9820 - loss: 0.0896 - precision: 0.8557 - recall: 0.9558 - tn: 123.8571 - tp: 88.0000 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0837 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 108/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9076 - auc: 0.9759 - loss: 0.0916 - precision: 0.8355 - recall: 0.9307 - tn: 131.3571 - tp: 81.7857 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0837 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 109/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9057 - auc: 0.9574 - loss: 0.0962 - precision: 0.8658 - recall: 0.9144 - tn: 125.7857 - tp: 86.3571 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0837 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 110/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8953 - auc: 0.9575 - loss: 0.0975 - precision: 0.8370 - recall: 0.9235 - tn: 121.2143 - tp: 85.9286 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0838 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 111/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8757 - auc: 0.9456 - loss: 0.0985 - precision: 0.8167 - recall: 0.8770 - tn: 124.5000 - tp: 80.7857 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0838 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 112/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9101 - auc: 0.9609 - loss: 0.0954 - precision: 0.8544 - recall: 0.9236 - tn: 130.2143 - tp: 81.4286 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0836 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 113/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9122 - auc: 0.9650 - loss: 0.0937 - precision: 0.8907 - recall: 0.8931 - tn: 128.4286 - tp: 83.0714 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0836 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 114/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9061 - auc: 0.9677 - loss: 0.0929 - precision: 0.8366 - recall: 0.9278 - tn: 129.6429 - tp: 80.7857 - val_accuracy: 0.9420 - val_auc: 0.9953 - val_loss: 0.0835 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 115/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9292 - auc: 0.9677 - loss: 0.0962 - precision: 0.8918 - recall: 0.9481 - tn: 123.2143 - tp: 90.9286 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0833 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 116/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9146 - auc: 0.9635 - loss: 0.0938 - precision: 0.9064 - recall: 0.8881 - tn: 128.0714 - tp: 84.3571 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0833 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 117/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8650 - auc: 0.9391 - loss: 0.1083 - precision: 0.8085 - recall: 0.8585 - tn: 123.5714 - tp: 78.7857 - val_accuracy: 0.9420 - val_auc: 0.9947 - val_loss: 0.0832 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 118/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9048 - auc: 0.9705 - loss: 0.0930 - precision: 0.8298 - recall: 0.9285 - tn: 132.1429 - tp: 81.2143 - val_accuracy: 0.9420 - val_auc: 0.9953 - val_loss: 0.0831 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 119/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8582 - auc: 0.9456 - loss: 0.0983 - precision: 0.7921 - recall: 0.8653 - tn: 123.1429 - tp: 81.0714 - val_accuracy: 0.9420 - val_auc: 0.9953 - val_loss: 0.0831 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 120/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8590 - auc: 0.9329 - loss: 0.1061 - precision: 0.8285 - recall: 0.8361 - tn: 122.5714 - tp: 80.3571 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0831 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 121/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9393 - auc: 0.9860 - loss: 0.0878 - precision: 0.8929 - recall: 0.9577 - tn: 132.1429 - tp: 84.7143 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0832 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 122/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9031 - auc: 0.9663 - loss: 0.0944 - precision: 0.8410 - recall: 0.9383 - tn: 121.6429 - tp: 86.3571 - val_accuracy: 0.9420 - val_auc: 0.9968 - val_loss: 0.0830 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 123/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9147 - auc: 0.9599 - loss: 0.1000 - precision: 0.8865 - recall: 0.9016 - tn: 129.3571 - tp: 82.2143 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0830 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 124/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9033 - auc: 0.9563 - loss: 0.0995 - precision: 0.8620 - recall: 0.9032 - tn: 128.2143 - tp: 83.0714 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0830 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 125/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9118 - auc: 0.9641 - loss: 0.0961 - precision: 0.9173 - recall: 0.8693 - tn: 127.4286 - tp: 83.2857 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0831 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 126/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8826 - auc: 0.9525 - loss: 0.0985 - precision: 0.8156 - recall: 0.8959 - tn: 123.4286 - tp: 83.0714 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0831 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 127/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9107 - auc: 0.9700 - loss: 0.0952 - precision: 0.8562 - recall: 0.9200 - tn: 130.0714 - tp: 80.4286 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0828 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 128/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9166 - auc: 0.9697 - loss: 0.1001 - precision: 0.8437 - recall: 0.9516 - tn: 129.9286 - tp: 84.3571 - val_accuracy: 0.9420 - val_auc: 0.9968 - val_loss: 0.0828 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 129/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9094 - auc: 0.9710 - loss: 0.0923 - precision: 0.8781 - recall: 0.9050 - tn: 126.5000 - tp: 84.2857 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0827 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 130/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9329 - auc: 0.9821 - loss: 0.0877 - precision: 0.9065 - recall: 0.9300 - tn: 129.5714 - tp: 86.1429 - val_accuracy: 0.9420 - val_auc: 0.9968 - val_loss: 0.0826 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 131/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8916 - auc: 0.9688 - loss: 0.0928 - precision: 0.8138 - recall: 0.9234 - tn: 127.8571 - tp: 81.0000 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0825 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 132/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8968 - auc: 0.9601 - loss: 0.0982 - precision: 0.8806 - recall: 0.8828 - tn: 124.8571 - tp: 85.4286 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0825 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 133/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8803 - auc: 0.9549 - loss: 0.0967 - precision: 0.7889 - recall: 0.9136 - tn: 126.7143 - tp: 79.5000 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0824 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 134/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9035 - auc: 0.9724 - loss: 0.0912 - precision: 0.8340 - recall: 0.9380 - tn: 125.5000 - tp: 84.9286 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0824 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 135/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9172 - auc: 0.9665 - loss: 0.0925 - precision: 0.8827 - recall: 0.9139 - tn: 130.2857 - tp: 84.2857 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0823 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 136/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9206 - auc: 0.9801 - loss: 0.0890 - precision: 0.8739 - recall: 0.9382 - tn: 125.4286 - tp: 86.9286 - val_accuracy: 0.9420 - val_auc: 0.9968 - val_loss: 0.0823 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 137/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9149 - auc: 0.9637 - loss: 0.0943 - precision: 0.8679 - recall: 0.9271 - tn: 127.9286 - tp: 85.7143 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0823 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 138/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8814 - auc: 0.9583 - loss: 0.0949 - precision: 0.8120 - recall: 0.9127 - tn: 122.6429 - tp: 83.6429 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0824 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 139/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8754 - auc: 0.9454 - loss: 0.0989 - precision: 0.8231 - recall: 0.8737 - tn: 125.2857 - tp: 82.0000 - val_accuracy: 0.9565 - val_auc: 0.9958 - val_loss: 0.0820 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 140/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8803 - auc: 0.9575 - loss: 0.0981 - precision: 0.7923 - recall: 0.9195 - tn: 126.8571 - tp: 81.0000 - val_accuracy: 0.9565 - val_auc: 0.9963 - val_loss: 0.0819 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 141/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8903 - auc: 0.9569 - loss: 0.0969 - precision: 0.8458 - recall: 0.9011 - tn: 122.5000 - tp: 84.2143 - val_accuracy: 0.9710 - val_auc: 0.9963 - val_loss: 0.0819 - val_precision: 0.9474 - val_recall: 0.9474 - val_tn: 49.0000 - val_tp: 18.0000\n",
      "Epoch 142/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9063 - auc: 0.9776 - loss: 0.0897 - precision: 0.8412 - recall: 0.9294 - tn: 130.2143 - tp: 83.5714 - val_accuracy: 0.9710 - val_auc: 0.9963 - val_loss: 0.0818 - val_precision: 0.9474 - val_recall: 0.9474 - val_tn: 49.0000 - val_tp: 18.0000\n",
      "Epoch 143/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9059 - auc: 0.9749 - loss: 0.0911 - precision: 0.8930 - recall: 0.8662 - tn: 130.7143 - tp: 79.6429 - val_accuracy: 0.9565 - val_auc: 0.9963 - val_loss: 0.0818 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 144/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8796 - auc: 0.9479 - loss: 0.0992 - precision: 0.8459 - recall: 0.8688 - tn: 120.8571 - tp: 84.5714 - val_accuracy: 0.9565 - val_auc: 0.9958 - val_loss: 0.0817 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 145/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9097 - auc: 0.9670 - loss: 0.0936 - precision: 0.8805 - recall: 0.9006 - tn: 129.8571 - tp: 84.4286 - val_accuracy: 0.9565 - val_auc: 0.9958 - val_loss: 0.0816 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 146/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9179 - auc: 0.9741 - loss: 0.0909 - precision: 0.8682 - recall: 0.9447 - tn: 123.7857 - tp: 90.2143 - val_accuracy: 0.9565 - val_auc: 0.9963 - val_loss: 0.0816 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 147/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8853 - auc: 0.9505 - loss: 0.0997 - precision: 0.8356 - recall: 0.8886 - tn: 124.2143 - tp: 82.7857 - val_accuracy: 0.9710 - val_auc: 0.9963 - val_loss: 0.0814 - val_precision: 0.9474 - val_recall: 0.9474 - val_tn: 49.0000 - val_tp: 18.0000\n",
      "Epoch 148/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9004 - auc: 0.9654 - loss: 0.0932 - precision: 0.8690 - recall: 0.8843 - tn: 129.2857 - tp: 82.2143 - val_accuracy: 0.9565 - val_auc: 0.9968 - val_loss: 0.0815 - val_precision: 0.9000 - val_recall: 0.9474 - val_tn: 48.0000 - val_tp: 18.0000\n",
      "Epoch 149/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9413 - auc: 0.9790 - loss: 0.0914 - precision: 0.9035 - recall: 0.9529 - tn: 131.7857 - tp: 86.9286 - val_accuracy: 0.9420 - val_auc: 0.9963 - val_loss: 0.0815 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n",
      "Epoch 150/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9081 - auc: 0.9788 - loss: 0.0904 - precision: 0.8437 - recall: 0.9288 - tn: 128.7143 - tp: 82.0714 - val_accuracy: 0.9420 - val_auc: 0.9958 - val_loss: 0.0815 - val_precision: 0.8571 - val_recall: 0.9474 - val_tn: 47.0000 - val_tp: 18.0000\n"
     ]
    }
   ],
   "source": [
    "# Definir los datos de entrenamiento base \n",
    "X_train_base = X_train_processed\n",
    "y_train_base = y_train\n",
    "\n",
    "# Función de Focal Loss para manejar desbalance\n",
    "def improved_focal_loss(gamma=2.5, alpha=0.65):\n",
    "    def focal_loss_fn(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Focal loss con cálculo más estable\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "        \n",
    "        # Balance de clases con alpha\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        \n",
    "        loss = modulating_factor * alpha_factor * cross_entropy\n",
    "        return tf.reduce_mean(loss)\n",
    "    return focal_loss_fn\n",
    "\n",
    "#Definir el modelo - USAR LA FORMA DE LOS DATOS BASE\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada\n",
    "model.add(Dense(\n",
    "    256, \n",
    "    input_shape=(X_train_base.shape[1],),\n",
    "    kernel_initializer=HeNormal(),  # Inicialización mejorada\n",
    "    kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)  # Regularización\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('swish'))  # Mejor que ReLU para muchos casos\n",
    "model.add(Dropout(0.6))  # Mayor dropout inicial\n",
    "\n",
    "# Capas ocultas\n",
    "model.add(Dense(\n",
    "    128,\n",
    "    kernel_initializer=HeNormal(),\n",
    "    kernel_regularizer=l1_l2(l1=1e-6, l2=1e-5)\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('swish'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(\n",
    "    64,\n",
    "    kernel_initializer=HeNormal(),\n",
    "    kernel_regularizer=l1_l2(l1=1e-6, l2=1e-5)\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('swish'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Capa adicional para mayor capacidad\n",
    "model.add(Dense(\n",
    "    32,\n",
    "    kernel_initializer=HeNormal(),\n",
    "    kernel_regularizer=l1_l2(l1=1e-6, l2=1e-5)\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('swish'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilación mejorada\n",
    "optimizer = Nadam(  # Mejor que Adam para muchos datasets\n",
    "    learning_rate=0.0001, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.999,\n",
    "    clipnorm=1.0  # Prevenir explosión de gradientes\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=improved_focal_loss(gamma=2.5, alpha=0.65),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        AUC(name='auc'),\n",
    "        TruePositives(name='tp'),  # Importante para clase minoritaria\n",
    "        TrueNegatives(name='tn')    # Importante para clase mayoritaria\n",
    "    ]\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train_base, \n",
    "    y_train_base,  \n",
    "    validation_split=0.15,  # Más datos para validación\n",
    "    epochs=150,  # Más épocas con paciencia extendida\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a776e7",
   "metadata": {
    "papermill": {
     "duration": 0.032857,
     "end_time": "2025-09-11T22:49:52.503345",
     "exception": false,
     "start_time": "2025-09-11T22:49:52.470488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### 1. Predictions\n",
    "We use the trained model to generate predictions on the **processed test set**:\n",
    "- `y_pred_prob`: Predicted probabilities (continuous values between 0 and 1).  \n",
    "- `y_pred_class`: Binary class predictions (0 = Benign, 1 = Malignant), using a threshold of 0.5.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Classification Report\n",
    "We evaluate the model with **precision, recall, F1-score, and accuracy**.  \n",
    "\n",
    "The results show:  \n",
    "- **Class 0 (Benign):** Precision = 0.99, Recall = 0.99, F1 = 0.99  \n",
    "- **Class 1 (Malignant):** Precision = 0.98, Recall = 0.98, F1 = 0.98  \n",
    "- **Overall Accuracy:** 98%  \n",
    "\n",
    "This indicates excellent performance in both detecting malignant cases (sensitivity) and correctly identifying benign cases (specificity).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Confusion Matrix\n",
    "A **confusion matrix** is plotted to visualize classification results:  \n",
    "- Rows: True labels  \n",
    "- Columns: Predicted labels  \n",
    "- Diagonal values represent correct predictions, while off-diagonal values represent misclassifications.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. ROC Curve and AUC\n",
    "We compute the **ROC (Receiver Operating Characteristic) curve** and **AUC (Area Under the Curve)**:  \n",
    "- The ROC curve plots **True Positive Rate (Recall)** vs. **False Positive Rate**.  \n",
    "- The AUC score is **0.983**, showing that the model has a strong ability to distinguish between benign and malignant tumors.  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "- The model achieves **98% accuracy** on the test set.  \n",
    "- Both classes are classified with very high precision and recall.  \n",
    "- The high **AUC score (0.983)** confirms that the model is robust and reliable for binary classification in this medical dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc6c6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:49:52.576694Z",
     "iopub.status.busy": "2025-09-11T22:49:52.576312Z",
     "iopub.status.idle": "2025-09-11T22:49:53.491296Z",
     "shell.execute_reply": "2025-09-11T22:49:53.489649Z"
    },
    "papermill": {
     "duration": 0.95469,
     "end_time": "2025-09-11T22:49:53.494435",
     "exception": false,
     "start_time": "2025-09-11T22:49:52.539745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "🔍 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        72\n",
      "           1       0.98      0.98      0.98        42\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.98      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Asegurar que usamos los datos procesados\n",
    "y_pred_prob = model.predict(X_test_processed).ravel()\n",
    "y_pred_class = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"🔍 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, [\"B\", \"M\"])\n",
    "plt.yticks(tick_marks, [\"B\", \"M\"])\n",
    "plt.ylabel('Etiqueta real')\n",
    "plt.xlabel('Predicción')\n",
    "\n",
    "# Etiquetas\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             ha=\"center\", va=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.show()\n",
    "\n",
    "# Curva ROC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"Curva ROC\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7975086,
     "sourceId": 12622449,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 83.67926,
   "end_time": "2025-09-11T22:49:56.796780",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-11T22:48:33.117520",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
